{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c56c182-95f6-4569-8b6f-4feac229208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all Required Data Sets and packages\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29fb10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (150, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Print the feature names\n",
    "#print(\"Feature names:\", iris.feature_names)\n",
    "\n",
    "# Print the target names\n",
    "#print(\"Target names:\", iris.target_names)\n",
    "\n",
    "# Print the first five rows of the data\n",
    "#print(\"Data:\\n\", iris.data[:5])\n",
    "\n",
    "# Print the first five target values\n",
    "#print(\"Target:\\n\", iris.target[:5])\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Data Shape:\", np.shape(iris.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1649fb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (178, 13)\n"
     ]
    }
   ],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Print the feature names\n",
    "#print(\"Feature names:\", wine.feature_names)\n",
    "\n",
    "# Print the target names\n",
    "#print(\"Target names:\", wine.target_names)\n",
    "\n",
    "# Print the first five rows of the data\n",
    "#print(\"Data:\\n\", wine.data[:5])\n",
    "\n",
    "# Print the first five target values\n",
    "#print(\"Target:\\n\", wine.target[:5])\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Data Shape:\", np.shape(wine.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a4f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Data Shape:\", np.shape(digits.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dee372e-c394-455e-bd95-667c39312493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class for a node on a tree\n",
    "\n",
    "class TreeNode:\n",
    "    #Constructor for a tree node. \n",
    "    #p must be the parent node (Tree Node)\n",
    "    #lc corresponds to the left child of the node (Tree Node)\n",
    "    #rc corresponds to the right child of the node (Tree Node)\n",
    "    #If the node is not a leaf node, f corresponds to the feature that the node will split on (int)\n",
    "    #If the node is not a leaf node, v corresponds to the value that the node will compare (float)\n",
    "    #If the node is a leaf node, c determines the class that the node ouputs (int)\n",
    "    #ln is a boolean variable which is true if the node is a leaf node (boolean)\n",
    "    \n",
    "    def __init__(self,p = None, lc = None, rc = None, f = None, v = None, c = None, ln = None):\n",
    "        #Stores the parent node if needed\n",
    "        self.parent = p\n",
    "        #Stores left child\n",
    "        self.leftChild = lc\n",
    "        #Stores right child\n",
    "        self.rightChild = rc\n",
    "        #Stores feature to check\n",
    "        self.feature = f\n",
    "        #Stores value to split on\n",
    "        self.value = v\n",
    "        #Stores classsification to return\n",
    "        self.classification = c\n",
    "        #Stores whether or not the node is a leaf node\n",
    "        self.isLeafNode = ln\n",
    "    #Setter methods for a variety of properties\n",
    "    def set_parent(self,p):\n",
    "        self.parent = p\n",
    "    def set_left_child(self,lc):\n",
    "        self.leftChild = lc\n",
    "    def set_right_child(self,rc):\n",
    "        self.rightChild = rc\n",
    "    def set_feature(self, f):\n",
    "        self.feature = f\n",
    "    def set_value(self, v):\n",
    "        self.value = v\n",
    "    def set_classification(self, c):\n",
    "        self.classification = c\n",
    "    #Returns true if node is a leaf node, false otherwise\n",
    "    def get_is_leaf_node(self):\n",
    "        return self.isLeafNode\n",
    "    #If the node is a leaf node, returns the class the node represents\n",
    "    def get_classification(self):\n",
    "        if(self.isLeafNode):\n",
    "            return self.classification\n",
    "        else:\n",
    "            return None\n",
    "    #If the node is not a leaf node, will return the next node in the tree for a specific observation\n",
    "    def bubble_down(self, observation):\n",
    "        if(not self.isLeafNode):\n",
    "            if(observation[self.feature] < self.value):\n",
    "                return self.leftChild\n",
    "            else:\n",
    "                return self.rightChild\n",
    "        else:\n",
    "            return None\n",
    "    #Getter Methods for various properties\n",
    "    def get_parent(self):\n",
    "        return self.parent\n",
    "    def get_right_child(self):\n",
    "        return self.rightChild\n",
    "    def get_left_child(self):\n",
    "        return self.rightChild\n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    def get_feature(self):\n",
    "        return self.feature\n",
    "    #Checks if a value goes to the left child or not\n",
    "    def goes_left(self, observation):\n",
    "        if(not self.isLeafNode):\n",
    "            if(observation[self.feature] < self.value):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903646cc-2a0e-4eaa-b849-23037ec0cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class for a whole tree\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    #If data and target are specified, builds a decision tree for that data and target\n",
    "    #r is the root node. If data is provided no root node is needed (TreeNode)\n",
    "    #md is the max depth the tree should reach. Default 0. (int)\n",
    "    #data is a 2-dimensional array with each row corresponding to an observation (2d_array)\n",
    "    #target is the class corresponding to the row in data. i.e. the tree should classify data[i] as target[i] (1d array)\n",
    "    #ms is the min samples that should be given to a leaf node. Default 1. (int)\n",
    "    \n",
    "    def __init__(self, r = None, md = 0, data = None, target = None, ms = 1):\n",
    "        #sets the max depth\n",
    "        self.maxDepth = md\n",
    "        #sets the min samples\n",
    "        self.minSamples = ms\n",
    "        #if data and target are not provided, nothing is constructed\n",
    "        if(data is None or target is None):\n",
    "            self.root = r\n",
    "        #if data and target are provided, builds decision tree\n",
    "        else:\n",
    "            self.root = self.build_tree(data,target)\n",
    "    \n",
    "    #Classifies an observation\n",
    "    def classify(self, observation):\n",
    "        #start at the root node\n",
    "        node = self.root\n",
    "        #bubble down the tree until a leaf node is reached\n",
    "        while(not node.get_is_leaf_node()):\n",
    "            node = node.bubble_down(observation)\n",
    "        #Classification of leaf node will be what the tree decided the class is\n",
    "        return node.get_classification()\n",
    "    #Revursive function to build the tree\n",
    "    def build_tree(self, data, target, depth = 0):\n",
    "        #If all the data has the same class, then the node doesn't have to split anymore and it becomes a leaf node\n",
    "        if(self.check_single_class(target)):\n",
    "            #Returns leaf node with classification as the only class in the sample\n",
    "            return TreeNode(ln = True, c = target[0])\n",
    "        #If max depth is reached, make a leaf node which classifies as the majority class\n",
    "        elif(depth == self.maxDepth):\n",
    "            return TreeNode(ln = True, c = self.get_most_probable_class(target))\n",
    "        #Calculates the best feature and value to split on and then generates two new nodes giving split data\n",
    "        #and target to the child nodes\n",
    "        else:\n",
    "            #Calculates the parent entropy\n",
    "            parent_entropy = self.get_entropy(target)\n",
    "            #Gets the best feature and value to split on\n",
    "            (feature,value) = self.get_feature_split(data,target,parent_entropy)\n",
    "            #If value was not assigned, there was no split that satisfied the min samples requirement\n",
    "            #and a leaf node is created.\n",
    "            if(value is None):\n",
    "                return TreeNode(ln = True, c = self.get_most_probable_class(target))\n",
    "            #Generates the new node in the tree\n",
    "            new_node = TreeNode(f = feature, v = value, ln = False)\n",
    "            #Splits the data based on the best feature and value found\n",
    "            (data1, target1, data2, target2) = self.split_data(new_node,data,target)\n",
    "            #Gives the data and target that go left to the build tree function,\n",
    "            #The returned node will be the left child\n",
    "            new_node.set_left_child(self.build_tree(data1,target1,depth = depth + 1))\n",
    "            #Gives the data and target that go right to the build tree function,\n",
    "            #The returned node will be the right child\n",
    "            new_node.set_right_child(self.build_tree(data2,target2,depth = depth + 1))\n",
    "            #Returns the node generated\n",
    "            return new_node\n",
    "    #Determines the best feature and value to split on\n",
    "    def get_feature_split(self,data,target,parent_entropy):\n",
    "        #The max information gained starts as smallest possible value\n",
    "        max_information_gain = float(\"-inf\")\n",
    "        #No feature or value assigned yet\n",
    "        feature = None\n",
    "        value = None\n",
    "        #Iterates over all observations\n",
    "        for i in range(len(data)):\n",
    "            #Iterates over the features\n",
    "            for j in range(len(data[i])):\n",
    "                #Assigns the current feature\n",
    "                feature_split = j\n",
    "                #Asssigns the current observations value\n",
    "                feature_value = data[i][j]\n",
    "                #Create a new node to split on the value\n",
    "                node = TreeNode(ln=False,f = feature_split, v = feature_value)\n",
    "                #Split the data\n",
    "                (data1,target1,data2,target2) = self.split_data(node,data,target)\n",
    "                #Check that the split gives at least the min samlples to the next node\n",
    "                if(len(target1) >= self.minSamples and len(target2) >= self.minSamples):\n",
    "                    #Computes the entropy in the left split\n",
    "                    left_entropy = self.get_entropy(target1)\n",
    "                    #Computes the entropy in the right split\n",
    "                    right_entropy = self.get_entropy(target2)\n",
    "                    #Computes the information gained with the current split\n",
    "                    information_gain = parent_entropy - (len(target1)*left_entropy+len(target2)*right_entropy)/len(target)\n",
    "                    #If this is the most information gained thus far, then split on this feature and this value\n",
    "                    if(information_gain > max_information_gain):\n",
    "                        feature = feature_split\n",
    "                        value = feature_value\n",
    "        #Returns feature and value that gives the most information gain\n",
    "        return (feature, value)\n",
    "    #Checks if the traget only has a single class in it\n",
    "    def check_single_class(self, target):\n",
    "        if(len(np.unique(target)) == 1):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    #Splits the data according to how a node would\n",
    "    def split_data(self, node,data,target):\n",
    "        #Data that goes left\n",
    "        data1 = []\n",
    "        #Data that goes right\n",
    "        data2 = []\n",
    "        #Target that goes left\n",
    "        target1 = []\n",
    "        #Target that goes right\n",
    "        target2 = []\n",
    "        #Iterates over the data\n",
    "        for i in range(len(data)):\n",
    "            left = node.goes_left(data[i])\n",
    "            #If data goes left add it to the left data array and the target as well\n",
    "            if(left):\n",
    "                data1.append(data[i])\n",
    "                target1.append(target[i])\n",
    "            #Otherwise data goes right and is added to the data that will go to the right\n",
    "            else:\n",
    "                data2.append(data[i])\n",
    "                target2.append(target[i])\n",
    "        #Returns the split data\n",
    "        return(np.array(data1),np.array(target1),np.array(data2),np.array(target2))\n",
    "    #Computes the most probable class\n",
    "    def get_most_probable_class(self,target):\n",
    "        data = Counter(target)\n",
    "        return data.most_common(1)[0][0]\n",
    "    #Computes the entropy in a target sample\n",
    "    def get_entropy(self, target):\n",
    "        # Calculate the proportion of samples in each class\n",
    "        p_i = np.unique(target,return_counts = True)[1]/len(target)\n",
    "        entropy = 0\n",
    "        # Calculate the entropy according to entropy = sum over all classes i (p_i*log_2(p_i))\n",
    "        for p in p_i:\n",
    "            if(p != 0):\n",
    "                entropy -= p * np.log2(p)\n",
    "        #returns the entropy\n",
    "        return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591a8679-6444-422b-a393-d7a5c525cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds a random forest\n",
    "\n",
    "class RandomForest:\n",
    "    #nt is the number of trees in the forest (int)\n",
    "    #md is the max depth for each of the trees (int)\n",
    "    #ms is the min samples that a leaf needs to have (int)\n",
    "    #data is an array of observations (2d-array)\n",
    "    #target are the corresponding classes to each observation (1d-array)\n",
    "    \n",
    "    def __init__(self,nt,md,ms,data,target):\n",
    "        #Gets the number of features each tree should consider\n",
    "        num_features = int(np.sqrt(np.shape(data)[1]))\n",
    "        #Generates a list of the trees in the forest\n",
    "        self.tree_list = [None]*nt\n",
    "        #Keeps track of the features each tree will split on\n",
    "        self.tree_features = [None]*nt\n",
    "        #Make trees equal to the trees in the forest\n",
    "        for i in range(nt):\n",
    "            #Gets random features to consider and random data\n",
    "            (tree_data,tree_target,tree_features) = self.GetRandomFeatureData(data,target,num_features)\n",
    "            #Keeps track of the tree features\n",
    "            self.tree_features[i] = tree_features\n",
    "            #Makes a new tree\n",
    "            self.tree_list[i] = DecisionTree(md = md, ms=ms, data = tree_data, target = tree_target)\n",
    "        return\n",
    "    #Gets random data for each tree\n",
    "    def GetRandomFeatureData(self,data,target,num_features):\n",
    "        #Get a random subset of features this tree will consider\n",
    "        random_features = [random.randint(0, np.shape(data)[1]-1) for _ in range(num_features)]\n",
    "        #Sort it and make it unique\n",
    "        unique_sorted_features = sorted(set(random_features))\n",
    "        #Generate a random list of indices to pull from data\n",
    "        random_data = [random.randint(0, np.shape(data)[0]-1) for _ in range(np.shape(data)[0])]\n",
    "        #Repeated data will only increase time, but won't make the tree better probably\n",
    "        random_data = sorted(set(random_data))\n",
    "        #Array for the random data\n",
    "        tree_data = []\n",
    "        tree_target = []\n",
    "        #For each observation to consider add it to the data\n",
    "        for i in range(len(random_data)):\n",
    "            row = []\n",
    "            #Only add the features to the row that will be considered by the tree\n",
    "            for j in range(len(unique_sorted_features)):\n",
    "                row.append(data[random_data[i]][unique_sorted_features[j]])\n",
    "            #Append data\n",
    "            tree_data.append(row)\n",
    "            tree_target.append(target[random_data[i]])                \n",
    "        return(tree_data,tree_target,unique_sorted_features)\n",
    "    #Classifies an observation by majority vote\n",
    "    def classify(self,observation):\n",
    "        #List of classifications\n",
    "        classifications = []\n",
    "        #Have each tree classify the observation\n",
    "        for i in range(len(self.tree_list)):\n",
    "            sample = []\n",
    "            #Get the features that the tree considers\n",
    "            for j in range(len(self.tree_features[i])):\n",
    "                sample.append(observation[self.tree_features[i][j]])\n",
    "            classifications.append(self.tree_list[i].classify(sample))\n",
    "        #Return most common class\n",
    "        top_class = Counter(classifications)\n",
    "        return top_class.most_common(1)[0][0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c3e3245-254d-47d8-8062-d2cb6bca5deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#Small test that the DecisionTree is working\n",
    "leftleaf = TreeNode(ln = True, c = 0)\n",
    "rightleaf = TreeNode(ln = True, c = 1)\n",
    "rootnode = TreeNode(lc = leftleaf, rc = rightleaf, f = 0, v = 0.5, ln = False)\n",
    "leftleaf.set_parent(rootnode)\n",
    "rightleaf.set_parent(rootnode)\n",
    "\n",
    "tree = DecisionTree(rootnode)\n",
    "\n",
    "print(tree.classify([1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf70707a-1945-4938-8368-244ed54074ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9666666666666667\n",
      "Correct:  29\n",
      "Incorrect:  1\n"
     ]
    }
   ],
   "source": [
    "#Iris dataset test to test single decision tree\n",
    "X_train, X_val, y_train, y_val = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "tree = DecisionTree(md = 14, data = X_train, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = tree.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b0cd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8333333333333334\n",
      "Correct:  30\n",
      "Incorrect:  6\n"
     ]
    }
   ],
   "source": [
    "#Wine Dataset test to test single decision tree\n",
    "X_train, X_val, y_train, y_val = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "tree = DecisionTree(md = 20, data = X_train, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = tree.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d488ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.37777777777777777\n",
      "Correct:  136\n",
      "Incorrect:  224\n"
     ]
    }
   ],
   "source": [
    "#Digits Dataset test to test single decision tree\n",
    "X_train, X_val, y_train, y_val = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "tree = DecisionTree(md = 5, data = X_train, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = tree.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75a84e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Correct:  30\n",
      "Incorrect:  0\n"
     ]
    }
   ],
   "source": [
    "#Iris dataset to test random forest\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "random_forest = RandomForest(data = X_train, nt = 50, md = 4, ms = 1, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = random_forest.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2255945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8888888888888888\n",
      "Correct:  32\n",
      "Incorrect:  4\n"
     ]
    }
   ],
   "source": [
    "#Wine datset to test RandomForest\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "random_forest = RandomForest(data = X_train, nt = 50, md = 4, ms = 1, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = random_forest.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24228e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6916666666666667\n",
      "Correct:  249\n",
      "Incorrect:  111\n"
     ]
    }
   ],
   "source": [
    "#Digits dataset to test RandomForest\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "random_forest = RandomForest(data = X_train, nt = 100, md = 4, ms = 1, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = random_forest.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42685412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
