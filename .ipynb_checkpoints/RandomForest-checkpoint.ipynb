{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7c56c182-95f6-4569-8b6f-4feac229208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all Required Data Sets\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a1ebc717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "Data:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Target:\n",
      " [0 0 0 0 0]\n",
      "Data Shape: (150, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Print the feature names\n",
    "print(\"Feature names:\", iris.feature_names)\n",
    "\n",
    "# Print the target names\n",
    "print(\"Target names:\", iris.target_names)\n",
    "\n",
    "# Print the first five rows of the data\n",
    "print(\"Data:\\n\", iris.data[:5])\n",
    "\n",
    "# Print the first five target values\n",
    "print(\"Target:\\n\", iris.target[:5])\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Data Shape:\", np.shape(iris.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1d4fdcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Target names: ['class_0' 'class_1' 'class_2']\n",
      "Data:\n",
      " [[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n",
      " [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n",
      "  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n",
      " [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      "  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n",
      "Target:\n",
      " [0 0 0 0 0]\n",
      "Data Shape: (178, 13)\n"
     ]
    }
   ],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Print the feature names\n",
    "print(\"Feature names:\", wine.feature_names)\n",
    "\n",
    "# Print the target names\n",
    "print(\"Target names:\", wine.target_names)\n",
    "\n",
    "# Print the first five rows of the data\n",
    "print(\"Data:\\n\", wine.data[:5])\n",
    "\n",
    "# Print the first five target values\n",
    "print(\"Target:\\n\", wine.target[:5])\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Data Shape:\", np.shape(wine.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "480a69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Data Shape:\", np.shape(digits.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4dee372e-c394-455e-bd95-667c39312493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class for a node on a tree\n",
    "\n",
    "class TreeNode:\n",
    "    #Constructor for a tree node. \n",
    "    #p must be the parent node (Tree Node)\n",
    "    #lc corresponds to the left child of the node (Tree Node)\n",
    "    #rc corresponds to the right child of the node (Tree Node)\n",
    "    #If the node is not a leaf node, f corresponds to the feature that the node will split on (int)\n",
    "    #If the node is not a leaf node, v corresponds to the value that the node will compare (float)\n",
    "    #If the node is a leaf node, c determines the class that the node ouputs (int)\n",
    "    #ln is a boolean variable which is true if the node is a leaf node (boolean)\n",
    "    \n",
    "    def __init__(self,p = None, lc = None, rc = None, f = None, v = None, c = None, ln = None):\n",
    "        #Stores the parent node if needed\n",
    "        self.parent = p\n",
    "        #Stores left child\n",
    "        self.leftChild = lc\n",
    "        #Stores right child\n",
    "        self.rightChild = rc\n",
    "        #Stores feature to check\n",
    "        self.feature = f\n",
    "        #Stores value to split on\n",
    "        self.value = v\n",
    "        #Stores classsification to return\n",
    "        self.classification = c\n",
    "        #Stores whether or not the node is a leaf node\n",
    "        self.isLeafNode = ln\n",
    "    #Setter methods for a variety of properties\n",
    "    def set_parent(self,p):\n",
    "        self.parent = p\n",
    "    def set_left_child(self,lc):\n",
    "        self.leftChild = lc\n",
    "    def set_right_child(self,rc):\n",
    "        self.rightChild = rc\n",
    "    def set_feature(self, f):\n",
    "        self.feature = f\n",
    "    def set_value(self, v):\n",
    "        self.value = v\n",
    "    def set_classification(self, c):\n",
    "        self.classification = c\n",
    "    #Returns true if node is a leaf node, false otherwise\n",
    "    def get_is_leaf_node(self):\n",
    "        return self.isLeafNode\n",
    "    #If the node is a leaf node, returns the class the node represents\n",
    "    def get_classification(self):\n",
    "        if(self.isLeafNode):\n",
    "            return self.classification\n",
    "        else:\n",
    "            return None\n",
    "    #If the node is not a leaf node, will return the next node in the tree for a specific observation\n",
    "    def bubble_down(self, observation):\n",
    "        if(not self.isLeafNode):\n",
    "            if(observation[self.feature] < self.value):\n",
    "                return self.leftChild\n",
    "            else:\n",
    "                return self.rightChild\n",
    "        else:\n",
    "            return None\n",
    "    #Getter Methods for various properties\n",
    "    def get_parent(self):\n",
    "        return self.parent\n",
    "    def get_right_child(self):\n",
    "        return self.rightChild\n",
    "    def get_left_child(self):\n",
    "        return self.rightChild\n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    def get_feature(self):\n",
    "        return self.feature\n",
    "    #Checks if a value goes to the left child or not\n",
    "    def goes_left(self, observation):\n",
    "        if(not self.isLeafNode):\n",
    "            if(observation[self.feature] < self.value):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "903646cc-2a0e-4eaa-b849-23037ec0cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, r = None, md = 0, data = None, target = None, ms = 1):\n",
    "        self.maxDepth = md\n",
    "        self.minSamples = ms\n",
    "        if(data is None or target is None):\n",
    "            self.root = r\n",
    "        else:\n",
    "            self.root = self.build_tree(data,target)\n",
    "    def classify(self, observation):\n",
    "        node = self.root\n",
    "        while(not node.get_is_leaf_node()):\n",
    "            node = node.bubble_down(observation)\n",
    "        return node.get_classification()\n",
    "    def build_tree(self, data, target, depth = 0):\n",
    "        if(self.check_single_class(target)):\n",
    "            return TreeNode(ln = True, c = target[0])\n",
    "        elif(depth == self.maxDepth):\n",
    "            return TreeNode(ln = True, c = self.get_most_probable_class(target))\n",
    "        else:\n",
    "            parent_entropy = self.get_entropy(target)\n",
    "            (feature,value) = self.get_feature_split(data,target,parent_entropy)\n",
    "            if(value is None):\n",
    "                return TreeNode(ln = True, c = self.get_most_probable_class(target))\n",
    "            new_node = TreeNode(f = feature, v = value, ln = False)\n",
    "            (data1, target1, data2, target2) = self.split_data(new_node,data,target)\n",
    "            new_node.set_left_child(self.build_tree(data1,target1,depth = depth + 1))\n",
    "            new_node.set_right_child(self.build_tree(data2,target2,depth = depth + 1))\n",
    "            return new_node\n",
    "    def get_feature_split(self,data,target,parent_entropy):\n",
    "        #needs to examine the data and determine the best value and feature to split at\n",
    "        max_information_gain = float(\"-inf\")\n",
    "        feature = None\n",
    "        value = None\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i])):\n",
    "                feature_split = j\n",
    "                feature_value = data[i][j]\n",
    "                node = TreeNode(ln=False,f = feature_split, v = feature_value)\n",
    "                (data1,target1,data2,target2) = self.split_data(node,data,target)\n",
    "                if(len(target1) >= self.minSamples and len(target2) >= self.minSamples):\n",
    "                    left_entropy = self.get_entropy(target1)\n",
    "                    right_entropy = self.get_entropy(target2)\n",
    "                    information_gain = parent_entropy - (len(target1)*left_entropy+len(target2)*right_entropy)/len(target)\n",
    "                    if(information_gain > max_information_gain):\n",
    "                        feature = feature_split\n",
    "                        value = feature_value\n",
    "        return (feature, value)\n",
    "    def check_single_class(self, target):\n",
    "        #Needs to check whether the target contains only one class\n",
    "        if(len(np.unique(target)) == 1):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def split_data(self, node,data,target):\n",
    "        #Needs to split the data to be passed down\n",
    "        data1 = []\n",
    "        data2 = []\n",
    "        target1 = []\n",
    "        target2 = []\n",
    "        for i in range(len(data)):\n",
    "            left = node.goes_left(data[i])\n",
    "            if(left):\n",
    "                data1.append(data[i])\n",
    "                target1.append(target[i])\n",
    "            else:\n",
    "                data2.append(data[i])\n",
    "                target2.append(target[i])\n",
    "        return(np.array(data1),np.array(target1),np.array(data2),np.array(target2))\n",
    "    def get_most_probable_class(self,target):\n",
    "        data = Counter(target)\n",
    "        return data.most_common(1)[0][0]\n",
    "    def get_entropy(self, target):\n",
    "        # Calculate the proportion of samples in each class\n",
    "        p_i = np.unique(target,return_counts = True)[1]/len(target)\n",
    "        entropy = 0\n",
    "        # Calculate the entropy\n",
    "        for p in p_i:\n",
    "            if(p != 0):\n",
    "                entropy -= p * np.log2(p)\n",
    "        return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4c3e3245-254d-47d8-8062-d2cb6bca5deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#Small test that the tree is working\n",
    "leftleaf = TreeNode(ln = True, c = 0)\n",
    "rightleaf = TreeNode(ln = True, c = 1)\n",
    "rootnode = TreeNode(lc = leftleaf, rc = rightleaf, f = 0, v = 0.5, ln = False)\n",
    "leftleaf.set_parent(rootnode)\n",
    "rightleaf.set_parent(rootnode)\n",
    "\n",
    "tree = DecisionTree(rootnode)\n",
    "\n",
    "print(tree.classify([1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cf70707a-1945-4938-8368-244ed54074ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9666666666666667\n",
      "Correct:  29\n",
      "Incorrect:  1\n"
     ]
    }
   ],
   "source": [
    "#Iris dataset test\n",
    "X_train, X_val, y_train, y_val = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "tree = DecisionTree(md = 14, data = X_train, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = tree.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "55eaf722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8333333333333334\n",
      "Correct:  30\n",
      "Incorrect:  6\n"
     ]
    }
   ],
   "source": [
    "#Wine Dataset test\n",
    "X_train, X_val, y_train, y_val = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "tree = DecisionTree(md = 20, data = X_train, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = tree.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "14af64d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.37777777777777777\n",
      "Correct:  136\n",
      "Incorrect:  224\n"
     ]
    }
   ],
   "source": [
    "#Digits Dataset test\n",
    "X_train, X_val, y_train, y_val = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "tree = DecisionTree(md = 5, data = X_train, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = tree.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "591a8679-6444-422b-a393-d7a5c525cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self,nt,md,ms,data,target):\n",
    "        num_features = int(np.sqrt(np.shape(data)[1]))\n",
    "        self.tree_list = [None]*nt\n",
    "        self.tree_features = [None]*nt\n",
    "        for i in range(nt):\n",
    "            (tree_data,tree_target,tree_features) = self.GetRandomFeatureData(data,target,num_features)\n",
    "            self.tree_features[i] = tree_features\n",
    "            self.tree_list[i] = DecisionTree(md = md, ms=ms, data = tree_data, target = tree_target)\n",
    "        return\n",
    "    def GetRandomFeatureData(self,data,target,num_features):\n",
    "        random_features = [random.randint(0, np.shape(data)[1]-1) for _ in range(num_features)]\n",
    "        unique_sorted_features = sorted(set(random_features))\n",
    "        random_data = [random.randint(0, np.shape(data)[0]-1) for _ in range(np.shape(data)[0])]\n",
    "        tree_data = []\n",
    "        tree_target = []\n",
    "        for i in range(len(random_data)):\n",
    "            row = []\n",
    "            for j in range(len(unique_sorted_features)):\n",
    "                row.append(data[random_data[i]][unique_sorted_features[j]])\n",
    "            tree_data.append(row)\n",
    "            tree_target.append(target[random_data[i]])                \n",
    "        return(tree_data,tree_target,unique_sorted_features)\n",
    "    def classify(self,observation):\n",
    "        classifications = []\n",
    "        for i in range(len(self.tree_list)):\n",
    "            sample = []\n",
    "            for j in range(len(self.tree_features[i])):\n",
    "                sample.append(observation[self.tree_features[i][j]])\n",
    "            classifications.append(self.tree_list[i].classify(sample))\n",
    "        top_class = Counter(classifications)\n",
    "        return top_class.most_common(1)[0][0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2f406306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9833333333333333\n",
      "Correct:  59\n",
      "Incorrect:  1\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "random_forest = RandomForest(data = X_train, nt = 50, md = 4, ms = 1, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = random_forest.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "996c965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9722222222222222\n",
      "Correct:  35\n",
      "Incorrect:  1\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "random_forest = RandomForest(data = X_train, nt = 50, md = 4, ms = 1, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = random_forest.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3e95b057",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(digits\u001b[38;5;241m.\u001b[39mdata, digits\u001b[38;5;241m.\u001b[39mtarget, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# X and y are your feature and target arrays, respectively.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# test_size specifies the proportion of the data that should be allocated to the validation set.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# random_state is used to ensure that the split is reproducible.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m random_forest \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m num_incorrect \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[131], line 9\u001b[0m, in \u001b[0;36mRandomForest.__init__\u001b[1;34m(self, nt, md, ms, data, target)\u001b[0m\n\u001b[0;32m      7\u001b[0m     (tree_data,tree_target,tree_features) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGetRandomFeatureData(data,target,num_features)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_features[i] \u001b[38;5;241m=\u001b[39m tree_features\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_list[i] \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtree_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtree_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[152], line 8\u001b[0m, in \u001b[0;36mDecisionTree.__init__\u001b[1;34m(self, r, md, data, target, ms)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m r\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[152], line 27\u001b[0m, in \u001b[0;36mDecisionTree.build_tree\u001b[1;34m(self, data, target, depth)\u001b[0m\n\u001b[0;32m     25\u001b[0m (data1, target1, data2, target2) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_data(new_node,data,target)\n\u001b[0;32m     26\u001b[0m new_node\u001b[38;5;241m.\u001b[39mset_left_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_tree(data1,target1,depth \u001b[38;5;241m=\u001b[39m depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 27\u001b[0m new_node\u001b[38;5;241m.\u001b[39mset_right_child(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_node\n",
      "Cell \u001b[1;32mIn[152], line 26\u001b[0m, in \u001b[0;36mDecisionTree.build_tree\u001b[1;34m(self, data, target, depth)\u001b[0m\n\u001b[0;32m     24\u001b[0m new_node \u001b[38;5;241m=\u001b[39m TreeNode(f \u001b[38;5;241m=\u001b[39m feature, v \u001b[38;5;241m=\u001b[39m value, ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m (data1, target1, data2, target2) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_data(new_node,data,target)\n\u001b[1;32m---> 26\u001b[0m new_node\u001b[38;5;241m.\u001b[39mset_left_child(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     27\u001b[0m new_node\u001b[38;5;241m.\u001b[39mset_right_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_tree(data2,target2,depth \u001b[38;5;241m=\u001b[39m depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_node\n",
      "Cell \u001b[1;32mIn[152], line 26\u001b[0m, in \u001b[0;36mDecisionTree.build_tree\u001b[1;34m(self, data, target, depth)\u001b[0m\n\u001b[0;32m     24\u001b[0m new_node \u001b[38;5;241m=\u001b[39m TreeNode(f \u001b[38;5;241m=\u001b[39m feature, v \u001b[38;5;241m=\u001b[39m value, ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m (data1, target1, data2, target2) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_data(new_node,data,target)\n\u001b[1;32m---> 26\u001b[0m new_node\u001b[38;5;241m.\u001b[39mset_left_child(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     27\u001b[0m new_node\u001b[38;5;241m.\u001b[39mset_right_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_tree(data2,target2,depth \u001b[38;5;241m=\u001b[39m depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_node\n",
      "Cell \u001b[1;32mIn[152], line 21\u001b[0m, in \u001b[0;36mDecisionTree.build_tree\u001b[1;34m(self, data, target, depth)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     parent_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_entropy(target)\n\u001b[1;32m---> 21\u001b[0m     (feature,value) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparent_entropy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m TreeNode(ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_most_probable_class(target))\n",
      "Cell \u001b[1;32mIn[152], line 39\u001b[0m, in \u001b[0;36mDecisionTree.get_feature_split\u001b[1;34m(self, data, target, parent_entropy)\u001b[0m\n\u001b[0;32m     37\u001b[0m feature_value \u001b[38;5;241m=\u001b[39m data[i][j]\n\u001b[0;32m     38\u001b[0m node \u001b[38;5;241m=\u001b[39m TreeNode(ln\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,f \u001b[38;5;241m=\u001b[39m feature_split, v \u001b[38;5;241m=\u001b[39m feature_value)\n\u001b[1;32m---> 39\u001b[0m (data1,target1,data2,target2) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(target1) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminSamples \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target2) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminSamples):\n\u001b[0;32m     41\u001b[0m     left_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_entropy(target1)\n",
      "Cell \u001b[1;32mIn[152], line 68\u001b[0m, in \u001b[0;36mDecisionTree.split_data\u001b[1;34m(self, node, data, target)\u001b[0m\n\u001b[0;32m     66\u001b[0m         data2\u001b[38;5;241m.\u001b[39mappend(data[i])\n\u001b[0;32m     67\u001b[0m         target2\u001b[38;5;241m.\u001b[39mappend(target[i])\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m)\u001b[49m,np\u001b[38;5;241m.\u001b[39marray(target1),np\u001b[38;5;241m.\u001b[39marray(data2),np\u001b[38;5;241m.\u001b[39marray(target2))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# X and y are your feature and target arrays, respectively.\n",
    "# test_size specifies the proportion of the data that should be allocated to the validation set.\n",
    "# random_state is used to ensure that the split is reproducible.\n",
    "\n",
    "random_forest = RandomForest(data = X_train, nt = 1000, md = 4, ms = 1, target = y_train)\n",
    "\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    output = random_forest.classify(X_val[i])\n",
    "    if(output == y_val[i]):\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        num_incorrect += 1\n",
    "print(\"Accuracy: \", num_correct/len(y_val))\n",
    "print(\"Correct: \", num_correct)\n",
    "print(\"Incorrect: \", num_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5caf05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
